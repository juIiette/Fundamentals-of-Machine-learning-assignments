{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Dataframe with all features by Juliette and Anke\n",
    "\n",
    "At the end the df contains:\n",
    "\n",
    "- Count of Adjectives and Adverbs. \n",
    "- Count of Entities.\n",
    "- All TextFeatures. \n",
    "- Readability: The Flesch Reading Ease formula.\n",
    "- Syllables count.\n",
    "- SpellChecker.\n",
    "- Punctuation count.\n",
    "- Uppercase & Lowercase count. \n",
    "- Sentiment analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Creating the dataframe with a sample of 2000 tweets. \n",
    "    This has been done for showing the steps of creating variables.\n",
    "    The analysis contains all of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_labeled.csv').sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Replacing binary labels with text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].replace(0, 'Fake', inplace=True)\n",
    "df['label'].replace(1, 'True', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"tweet_id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anke: Cleaning tweets\n",
    "I removed the mentions of retweets ‚ÄúRT @account_name\" and the https addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    txt = df.loc[i][\"text\"]\n",
    "    txt = re.sub(r\"RT\\ \\@\\w*\\:\\ \", '', txt) #replace RT-tags\n",
    "    txt= re.sub(r'@[A-Z0-9a-z_:]+','',txt) #replace username-tags\n",
    "    txt = re.sub('https?://[A-Za-z0-9./]+','',txt) #replace URLs\n",
    "    df.at[i,\"text\"]=txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anke: Adjectives and Adverbs\n",
    "Using Python‚Äôs natural language processing (NLP) library spaCy I calculated the adjectives and adverbs; two grammatical concepts that are there to describe and modify nouns and verbs, respectively.\n",
    "https://spacy.io/usage/spacy-101 \n",
    "\n",
    "Part-of-speech (POS) Tagging\tAssigning word types to tokens, like verb or noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model via spacy.load(). This will return a Language object containing all components and data needed to process text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linguistic annotations are available as Token attributes. Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name. \n",
    "\n",
    "- .pos = int = Coarse-grained part-of-speech from the Universal POS tag set.\n",
    "- .pos_ = unicode = Coarse-grained part-of-speech from the Universal POS tag set.\n",
    "\n",
    "Spacy is highly optimised and does the multiprocessing for you. So it is better to take the data out of a df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use the spacy pipeline I created a list \n",
    "pos = []\n",
    "\n",
    "for doc in nlp.pipe(df['text'].astype('unicode').values):\n",
    "    if doc.is_parsed:\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the same number of entries of the original Dataframe.\n",
    "        # Add blanks in case the parse fails\n",
    "        pos.append(None)\n",
    "\n",
    "df = df.assign(PartofSpeach = pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alphabetical listing Universal POS tags\n",
    "These tags mark the core part-of-speech categories. To distinguish additional lexical and grammatical properties of words, use the universal features.\n",
    "source:https://universaldependencies.org/docs/u/pos/\n",
    "\n",
    "- <b> ADJ: adjective </b>\n",
    "- ADP: adposition\n",
    "- <b> ADV: adverb </b>\n",
    "- AUX: auxiliary verb\n",
    "- CONJ: coordinating conjunction\n",
    "- DET: determiner\n",
    "- INTJ: interjection\n",
    "- NOUN: noun\n",
    "- NUM: numeral\n",
    "- PART: particle\n",
    "- PRON: pronoun\n",
    "- PROPN: proper noun\n",
    "- PUNCT: punctuation\n",
    "- SCONJ: subordinating conjunction\n",
    "- SYM: symbol\n",
    "- VERB: verb\n",
    "- X: other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# all Universal POS tags can be entered here\n",
    "grammarList = ['ADJ', 'ADV', 'ADP', 'AUX', 'CONJ', \"DET\", \"INTJ\", 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    c = Counter(([token.pos_ for token in nlp(row['text'])]))\n",
    "    for el, cnt in c.items():\n",
    "        if el in grammarList:\n",
    "            df.loc[index, el] = cnt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anke: Entities\n",
    "\n",
    "from: https://texthero.org/docs/api/texthero.nlp.named_entities\n",
    "*named_entities(s, package='spacy')*\n",
    "\n",
    "Return named-entities.\n",
    "Return a Pandas Series where each rows contains a list of tuples containing information regarding the given named entities.\n",
    "\n",
    "Tuple: *(entity‚Äôname, entity‚Äôlabel, starting character, ending character)*\n",
    "\n",
    "Under the hood, named_entities make use of Spacy name entity recognition.\n",
    "\n",
    "List of labels:\n",
    "- PERSON: People, including fictional.\n",
    "- NORP: Nationalities or religious or political groups.\n",
    "- FAC: Buildings, airports, highways, bridges, etc.\n",
    "- ORG : Companies, agencies, institutions, etc.\n",
    "- GPE: Countries, cities, states.\n",
    "- LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- PRODUCT: Objects, vehicles, foods, etc. (Not services.)\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LAW: Named documents made into laws.\n",
    "- LANGUAGE: Any named language.\n",
    "- DATE: Absolute or relative dates or periods.\n",
    "- TIME: Times smaller than a day.\n",
    "- PERCENT: Percentage, including ‚Äù%‚Äú.\n",
    "- MONEY: Monetary values, including unit.\n",
    "- QUANTITY: Measurements, as of weight or distance.\n",
    "- ORDINAL: ‚Äúfirst‚Äù, ‚Äúsecond‚Äù, etc.\n",
    "- CARDINAL: Numerals that do not fall under another type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = hero.named_entities(df['text'])\n",
    "df['entities'] = entities\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_list = []\n",
    "\n",
    "# loop through array\n",
    "for index, row in df.iterrows():\n",
    "    # loop through object\n",
    "    for item in row['entities']:\n",
    "        # item [1] is second item in object so the entitie\n",
    "        if item[1] not in entities_list:\n",
    "            # add to entities_list\n",
    "            entities_list.append(item[1])\n",
    "            # create column and set first appearance of entitie to 1\n",
    "            df.loc[index, item[1]] = 1\n",
    "        # change NaN to 1  \n",
    "        # math checks if value is NaN\n",
    "        elif math.isnan(df.loc[index, item[1]]):\n",
    "            df.loc[index, item[1]] = 1\n",
    "        # add to number\n",
    "        elif item[1] in entities_list and not math.isnan(df.loc[index, item[1]]):\n",
    "            df.loc[index, item[1]] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everyone: Textfeatures\n",
    "\n",
    "Textfeatures is a python package which extracts the basic features from the text data such as hashtags, stopwords, numerics. \n",
    "\n",
    "In this file we will inspect the dataset from an AI research project using: \n",
    "\n",
    "1. word_count():- give the total words count present in text data.\n",
    "\n",
    "2. char_count():- give the characters count.\n",
    "\n",
    "3. avg_word_length():- give the average word length.\n",
    "\n",
    "4. stopwords_count():- give the stopwords count.\n",
    "\n",
    "5. stopwords():- extract the stopwords from the text data.\n",
    "\n",
    "6. hashtags_count():- give the hashtags count.\n",
    "\n",
    "7. hashtags():- extract the hashtags from text data.\n",
    "\n",
    "8. numeric_count():- give the numeric digits count.\n",
    "\n",
    "9. user_mentions_count():- give the user mentions count from text data.\n",
    "\n",
    "10. user_mentions():- extract the user mentions from text data.\n",
    "\n",
    "11. clean():- give the pre-processed data after removal for unnecessary material in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>PartofSpeach</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>VERB</th>\n",
       "      <th>AUX</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>PRON</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PART</th>\n",
       "      <th>NUM</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>ADV</th>\n",
       "      <th>X</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>entities</th>\n",
       "      <th>CARDINAL</th>\n",
       "      <th>ORG</th>\n",
       "      <th>GPE</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>MONEY</th>\n",
       "      <th>WORK_OF_ART</th>\n",
       "      <th>NORP</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>TIME</th>\n",
       "      <th>PERCENT</th>\n",
       "      <th>FAC</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ORDINAL</th>\n",
       "      <th>LAW</th>\n",
       "      <th>LOC</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>num_count</th>\n",
       "      <th>user_mentions_count</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155520</th>\n",
       "      <td>'#MoscowMitch should be the focus of everyone ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[PUNCT, PROPN, PROPN, VERB, AUX, DET, NOUN, AD...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(#, CARDINAL, 1, 2), (MoscowMitch, ORG, 2, 13...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>121</td>\n",
       "      <td>5.368421</td>\n",
       "      <td>9</td>\n",
       "      <td>[should, be, the, of, who, about, the, of, the]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>moscowmitch focus everyone cares america forge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210971</th>\n",
       "      <td>'Not sure my grandparents would‚Äôve been allowe...</td>\n",
       "      <td>True</td>\n",
       "      <td>[PUNCT, PART, ADJ, DET, NOUN, VERB, VERB, AUX,...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>120</td>\n",
       "      <td>5.368421</td>\n",
       "      <td>6</td>\n",
       "      <td>[my, been, to, by, this, and]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>sure grandparents wouldve allowed enter measur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186412</th>\n",
       "      <td>'Excellent piece: üëáüèª‚ÄúCIA Prepares the Blame th...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>[PUNCT, ADJ, NOUN, PUNCT, NUM, NOUN, VERB, DET...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(üëá, CARDINAL, 18, 19)]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>116</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>[the, the, for, the, the]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>excellent piece prepares blame consultant excu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206715</th>\n",
       "      <td>'Illegal Immigration Expected to Hit Highest L...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>[PUNCT, ADJ, NOUN, VERB, PART, VERB, ADJ, PROP...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(George W. Bush', PERSON, 57, 72)]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>72</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>1</td>\n",
       "      <td>[to]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>illegal immigration expected highest level sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207516</th>\n",
       "      <td>'Previously Deported Sex Offenders Hiding in M...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>[PUNCT, ADV, VERB, NOUN, NOUN, PROPN, ADP, PRO...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>72</td>\n",
       "      <td>5.545455</td>\n",
       "      <td>1</td>\n",
       "      <td>[in]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>previously deported offenders hiding migrant g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106049</th>\n",
       "      <td>'Donald Trump Jr.: Covington Catholic Hoax Sho...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>[PUNCT, PROPN, PROPN, PROPN, PUNCT, PROPN, PRO...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[('Donald Trump Jr., PERSON, 0, 17), (Covingto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>105</td>\n",
       "      <td>4.578947</td>\n",
       "      <td>2</td>\n",
       "      <td>[the, of]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>donald trump covington catholic hoax shows rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29130</th>\n",
       "      <td>'Ugh, too real '</td>\n",
       "      <td>Fake</td>\n",
       "      <td>[PUNCT, NOUN, PUNCT, ADV, ADJ, PUNCT]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>[too]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8357</th>\n",
       "      <td>'A newly unearthed letter from 2016 shows that...</td>\n",
       "      <td>True</td>\n",
       "      <td>[PUNCT, DET, ADV, VERB, NOUN, ADP, NUM, VERB, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(2016, DATE, 31, 35), (Republican, NORP, 47, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>124</td>\n",
       "      <td>5.944444</td>\n",
       "      <td>4</td>\n",
       "      <td>[from, that, for, to]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>newly unearthed letter shows republican senato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137882</th>\n",
       "      <td>'Feds Resettle 224K Border Crossers Across U.S...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>[PUNCT, PROPN, NOUN, NUM, PROPN, PROPN, PROPN,...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>66</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>feds resettle border crossers across every hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134446</th>\n",
       "      <td>'\"Wear your MAGA hats with pride, kids‚Äù writes...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>[PUNCT, PUNCT, VERB, DET, NOUN, NOUN, ADP, NOU...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[(//t.co/B0a21c9BKQ', ORG, 52, 70)]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>70</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>[your, with, for]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>wear maga hats pride kids writes tcobacbkq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text label  \\\n",
       "155520  '#MoscowMitch should be the focus of everyone ...  True   \n",
       "210971  'Not sure my grandparents would‚Äôve been allowe...  True   \n",
       "186412  'Excellent piece: üëáüèª‚ÄúCIA Prepares the Blame th...  Fake   \n",
       "206715  'Illegal Immigration Expected to Hit Highest L...  Fake   \n",
       "207516  'Previously Deported Sex Offenders Hiding in M...  Fake   \n",
       "...                                                   ...   ...   \n",
       "106049  'Donald Trump Jr.: Covington Catholic Hoax Sho...  Fake   \n",
       "29130                                    'Ugh, too real '  Fake   \n",
       "8357    'A newly unearthed letter from 2016 shows that...  True   \n",
       "137882  'Feds Resettle 224K Border Crossers Across U.S...  Fake   \n",
       "134446  '\"Wear your MAGA hats with pride, kids‚Äù writes...  Fake   \n",
       "\n",
       "                                             PartofSpeach  PUNCT  PROPN  VERB  \\\n",
       "155520  [PUNCT, PROPN, PROPN, VERB, AUX, DET, NOUN, AD...    5.0    5.0   3.0   \n",
       "210971  [PUNCT, PART, ADJ, DET, NOUN, VERB, VERB, AUX,...    4.0    NaN   6.0   \n",
       "186412  [PUNCT, ADJ, NOUN, PUNCT, NUM, NOUN, VERB, DET...    5.0    4.0   2.0   \n",
       "206715  [PUNCT, ADJ, NOUN, VERB, PART, VERB, ADJ, PROP...    2.0    4.0   2.0   \n",
       "207516  [PUNCT, ADV, VERB, NOUN, NOUN, PROPN, ADP, PRO...    3.0    4.0   2.0   \n",
       "...                                                   ...    ...    ...   ...   \n",
       "106049  [PUNCT, PROPN, PROPN, PROPN, PUNCT, PROPN, PRO...    4.0   11.0   1.0   \n",
       "29130               [PUNCT, NOUN, PUNCT, ADV, ADJ, PUNCT]    3.0    NaN   NaN   \n",
       "8357    [PUNCT, DET, ADV, VERB, NOUN, ADP, NUM, VERB, ...    3.0    1.0   3.0   \n",
       "137882  [PUNCT, PROPN, NOUN, NUM, PROPN, PROPN, PROPN,...    2.0    7.0   NaN   \n",
       "134446  [PUNCT, PUNCT, VERB, DET, NOUN, NOUN, ADP, NOU...    5.0    2.0   2.0   \n",
       "\n",
       "        AUX  DET  NOUN  ADP  PRON  ADJ  PART  NUM  SCONJ  ADV   X  INTJ  SYM  \\\n",
       "155520  1.0  3.0   2.0  3.0   2.0  1.0   NaN  NaN    NaN  NaN NaN   NaN  NaN   \n",
       "210971  1.0  2.0   4.0  1.0   1.0  1.0   2.0  NaN    NaN  NaN NaN   NaN  NaN   \n",
       "186412  NaN  4.0   4.0  2.0   NaN  1.0   NaN  1.0    NaN  NaN NaN   NaN  NaN   \n",
       "206715  NaN  NaN   1.0  NaN   NaN  2.0   1.0  NaN    1.0  NaN NaN   NaN  NaN   \n",
       "207516  NaN  NaN   2.0  1.0   NaN  NaN   NaN  NaN    NaN  1.0 NaN   NaN  NaN   \n",
       "...     ...  ...   ...  ...   ...  ...   ...  ...    ...  ...  ..   ...  ...   \n",
       "106049  1.0  1.0   1.0  1.0   1.0  1.0   NaN  NaN    NaN  NaN NaN   NaN  NaN   \n",
       "29130   NaN  NaN   1.0  NaN   NaN  1.0   NaN  NaN    NaN  1.0 NaN   NaN  NaN   \n",
       "8357    NaN  1.0   5.0  4.0   NaN  1.0   2.0  1.0    1.0  1.0 NaN   NaN  NaN   \n",
       "137882  NaN  1.0   2.0  NaN   NaN  NaN   NaN  1.0    NaN  NaN NaN   NaN  NaN   \n",
       "134446  NaN  1.0   4.0  2.0   NaN  NaN   NaN  NaN    NaN  NaN NaN   NaN  1.0   \n",
       "\n",
       "                                                 entities  CARDINAL  ORG  GPE  \\\n",
       "155520  [(#, CARDINAL, 1, 2), (MoscowMitch, ORG, 2, 13...       1.0  2.0  1.0   \n",
       "210971                                                 []       NaN  NaN  NaN   \n",
       "186412                            [(üëá, CARDINAL, 18, 19)]       1.0  NaN  NaN   \n",
       "206715                [(George W. Bush', PERSON, 57, 72)]       NaN  NaN  NaN   \n",
       "207516                                                 []       NaN  NaN  NaN   \n",
       "...                                                   ...       ...  ...  ...   \n",
       "106049  [('Donald Trump Jr., PERSON, 0, 17), (Covingto...       NaN  NaN  1.0   \n",
       "29130                                                  []       NaN  NaN  NaN   \n",
       "8357    [(2016, DATE, 31, 35), (Republican, NORP, 47, ...       NaN  NaN  1.0   \n",
       "137882                                                 []       NaN  NaN  NaN   \n",
       "134446                [(//t.co/B0a21c9BKQ', ORG, 52, 70)]       NaN  1.0  NaN   \n",
       "\n",
       "        PERSON  MONEY  WORK_OF_ART  NORP  EVENT  TIME  PERCENT  FAC  DATE  \\\n",
       "155520     NaN    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "210971     NaN    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "186412     NaN    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "206715     1.0    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "207516     NaN    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "...        ...    ...          ...   ...    ...   ...      ...  ...   ...   \n",
       "106049     2.0    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "29130      NaN    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "8357       NaN    NaN          NaN   1.0    NaN   NaN      NaN  NaN   1.0   \n",
       "137882     NaN    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "134446     NaN    NaN          NaN   NaN    NaN   NaN      NaN  NaN   NaN   \n",
       "\n",
       "        ORDINAL  LAW  LOC  PRODUCT  QUANTITY  LANGUAGE  word_count  \\\n",
       "155520      NaN  NaN  NaN      NaN       NaN       NaN          20   \n",
       "210971      NaN  NaN  NaN      NaN       NaN       NaN          19   \n",
       "186412      NaN  NaN  NaN      NaN       NaN       NaN          18   \n",
       "206715      NaN  NaN  NaN      NaN       NaN       NaN          11   \n",
       "207516      NaN  NaN  NaN      NaN       NaN       NaN          12   \n",
       "...         ...  ...  ...      ...       ...       ...         ...   \n",
       "106049      NaN  NaN  NaN      NaN       NaN       NaN          19   \n",
       "29130       NaN  NaN  NaN      NaN       NaN       NaN           4   \n",
       "8357        NaN  NaN  NaN      NaN       NaN       NaN          18   \n",
       "137882      NaN  NaN  NaN      NaN       NaN       NaN          10   \n",
       "134446      NaN  NaN  NaN      NaN       NaN       NaN          11   \n",
       "\n",
       "        char_count  avg_word_length  stopwords_count  \\\n",
       "155520         121         5.368421                9   \n",
       "210971         120         5.368421                6   \n",
       "186412         116         5.500000                5   \n",
       "206715          72         5.636364                1   \n",
       "207516          72         5.545455                1   \n",
       "...            ...              ...              ...   \n",
       "106049         105         4.578947                2   \n",
       "29130           16         3.250000                1   \n",
       "8357           124         5.944444                4   \n",
       "137882          66         5.700000                0   \n",
       "134446          70         6.000000                3   \n",
       "\n",
       "                                              stopwords  hashtags_count  \\\n",
       "155520  [should, be, the, of, who, about, the, of, the]               0   \n",
       "210971                    [my, been, to, by, this, and]               0   \n",
       "186412                        [the, the, for, the, the]               0   \n",
       "206715                                             [to]               0   \n",
       "207516                                             [in]               0   \n",
       "...                                                 ...             ...   \n",
       "106049                                        [the, of]               0   \n",
       "29130                                             [too]               0   \n",
       "8357                              [from, that, for, to]               0   \n",
       "137882                                               []               0   \n",
       "134446                                [your, with, for]               0   \n",
       "\n",
       "       hashtags  num_count  user_mentions_count user_mentions  \\\n",
       "155520       []          0                    0            []   \n",
       "210971       []          0                    0            []   \n",
       "186412       []          0                    0            []   \n",
       "206715       []          0                    0            []   \n",
       "207516       []          0                    0            []   \n",
       "...         ...        ...                  ...           ...   \n",
       "106049       []          0                    0            []   \n",
       "29130        []          0                    0            []   \n",
       "8357         []          1                    0            []   \n",
       "137882       []          0                    0            []   \n",
       "134446       []          0                    0            []   \n",
       "\n",
       "                                               clean_text  \n",
       "155520  moscowmitch focus everyone cares america forge...  \n",
       "210971  sure grandparents wouldve allowed enter measur...  \n",
       "186412  excellent piece prepares blame consultant excu...  \n",
       "206715  illegal immigration expected highest level sin...  \n",
       "207516  previously deported offenders hiding migrant g...  \n",
       "...                                                   ...  \n",
       "106049  donald trump covington catholic hoax shows rea...  \n",
       "29130                                                real  \n",
       "8357    newly unearthed letter shows republican senato...  \n",
       "137882  feds resettle border crossers across every hal...  \n",
       "134446         wear maga hats pride kids writes tcobacbkq  \n",
       "\n",
       "[2000 rows x 49 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textfeatures as tf\n",
    "\n",
    "tf.word_count(df,\"text\",\"word_count\")\n",
    "tf.char_count(df,\"text\",\"char_count\")\n",
    "tf.avg_word_length(df,\"text\",\"avg_word_length\")\n",
    "tf.stopwords_count(df,\"text\",\"stopwords_count\")\n",
    "tf.stopwords(df,\"text\",\"stopwords\")\n",
    "tf.hashtags_count(df,\"text\",\"hashtags_count\")\n",
    "tf.hashtags(df,\"text\",\"hashtags\")\n",
    "tf.numerics_count(df,\"text\",\"num_count\")\n",
    "tf.user_mentions_count(df,\"text\",\"user_mentions_count\")\n",
    "tf.user_mentions(df,\"text\",\"user_mentions\")\n",
    "tf.clean(df,\"text\",\"clean_text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juliette: The Flesch Reading Ease formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Textstat has been used, which is a library to calculate statistics from text. \n",
    "    It helps determine readability, complexity, and grade level.\n",
    "    \n",
    "    The following formula returns the Flesch Reading Ease Score (FRES). \n",
    "    It scores on the readability of the document.\n",
    "    \n",
    "    While the maximum score is 121.22, there is no limit on how low the score can be. \n",
    "    A negative score is valid.\n",
    "    \n",
    "The documentation can be found on https://pypi.org/project/textstat/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Score Difficulty\n",
    "    90-100 Very Easy\n",
    "    80-89 Easy\n",
    "    70-79 Fairly Easy\n",
    "    60-69 Standard\n",
    "    50-59 Fairly Difficult\n",
    "    30-49 Difficult\n",
    "    0-29 Very Confusing\n",
    "\n",
    "Flesch, Rudolf. \"How to Write Plain English\". University of Canterbury. Archived from the original on July 12, 2016. Retrieved July 12, 2016. https://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FRES']=df['text'].apply(textstat.flesch_reading_ease).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juliette: Syllables count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    With textstat, I have also counted syllables in the text. \n",
    "    The following formula returns the number of syllables present in the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Syllables']=df['text'].apply(textstat.syllable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juliette: Sentences count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Textstat also has a possibility to count the sentences in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentences']=df['text'].apply(textstat.sentence_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juliette: SpellChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    With pyspellchecker, I performed Pure Python Spell Checking based on Peter Norvig‚Äôs blog post.\n",
    "    It uses a Levenshtein Distance algorithm to find permutations,\n",
    "    within an edit distance of 2 from the original word. \n",
    "    It then compares all permutations (insertions, deletions, replacements, and transpositions) \n",
    "    to known words in a word frequency list. \n",
    "    Those words that are found more often in the frequency list are more likely the correct results.\n",
    "    \n",
    "Documentation: https://pypi.org/project/pyspellchecker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "df['Spelling_Mistakes']=df['text'].apply(spell.unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juliette: punctuation count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For the punctuation count, I used a code that I found on stack overflow: \n",
    "https://stackoverflow.com/questions/58252056/count-punctuation-in-a-dataframe-column\n",
    "\n",
    "    First, I count the number of punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "df['count_punct'] = df.text.apply(lambda s: count(s, string.punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Then I create a list where all the punctuations of a tweet are accumulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate = lambda l1,l2: [x for x in l1 if x in l2]\n",
    "\n",
    "df['acc_punct_list'] = df.text.apply(lambda s: accumulate(s, string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The next step is to create a dictionary that counts the occurences of each punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['acc_punct_dict'] = df.text.apply(lambda s: {k:v for k, v in Counter(s).items() if k in string.punctuation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now the punctuations will each be given a column of their own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_punct = df.acc_punct_dict.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    And at last, the column will be merged with the entire dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_punct], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juliette: Uppercase & Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To count the uppercases and lowercases in the tweets, I found a piece of code in stack overflow.\n",
    "https://stackoverflow.com/questions/49230262/how-to-count-uppercase-and-lowercase-on-pandas-dataframe\n",
    "\n",
    "    It finds all the upper- or lowercases, puts them in a list, and then counts the length of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Uppercase'] = df['text'].str.findall(r'[A-Z]').str.len()\n",
    "df['Lowercase'] = df['text'].str.findall(r'[a-z]').str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The dataframe of my added features will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Spelling_Mistakes</th>\n",
       "      <th>count_punct</th>\n",
       "      <th>acc_punct_list</th>\n",
       "      <th>acc_punct_dict</th>\n",
       "      <th>'</th>\n",
       "      <th>#</th>\n",
       "      <th>.</th>\n",
       "      <th>,</th>\n",
       "      <th>:</th>\n",
       "      <th>-</th>\n",
       "      <th>!</th>\n",
       "      <th>$</th>\n",
       "      <th>|</th>\n",
       "      <th>?</th>\n",
       "      <th>/</th>\n",
       "      <th>%</th>\n",
       "      <th>\"</th>\n",
       "      <th>(</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>=</th>\n",
       "      <th>@</th>\n",
       "      <th>)</th>\n",
       "      <th>\\</th>\n",
       "      <th>[</th>\n",
       "      <th>]</th>\n",
       "      <th>*</th>\n",
       "      <th>+</th>\n",
       "      <th>Uppercase</th>\n",
       "      <th>Lowercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155520</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>{‚Ä¶,  }</td>\n",
       "      <td>5</td>\n",
       "      <td>[', #, ., ,, ']</td>\n",
       "      <td>{''': 2, '#': 1, '.': 1, ',': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210971</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>{‚Äô,  , ‚Ä¶}</td>\n",
       "      <td>3</td>\n",
       "      <td>[', ., ']</td>\n",
       "      <td>{''': 2, '.': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186412</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>{‚Ä¶,  , ‚Äú, üèª, üëá, ‚Äù}</td>\n",
       "      <td>3</td>\n",
       "      <td>[', :, ']</td>\n",
       "      <td>{''': 2, ':': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206715</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>{ }</td>\n",
       "      <td>3</td>\n",
       "      <td>[', ., ']</td>\n",
       "      <td>{''': 2, '.': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207516</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>{ }</td>\n",
       "      <td>3</td>\n",
       "      <td>[', ,, ']</td>\n",
       "      <td>{''': 2, ',': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106049</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>{‚Ä¶,  }</td>\n",
       "      <td>5</td>\n",
       "      <td>[', ., :, |, ']</td>\n",
       "      <td>{''': 2, '.': 1, ':': 1, '|': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29130</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>{ }</td>\n",
       "      <td>3</td>\n",
       "      <td>[', ,, ']</td>\n",
       "      <td>{''': 2, ',': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8357</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>{‚Ä¶,  }</td>\n",
       "      <td>4</td>\n",
       "      <td>[', ', ', ']</td>\n",
       "      <td>{''': 4}</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137882</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>{ }</td>\n",
       "      <td>4</td>\n",
       "      <td>[', ., ., ']</td>\n",
       "      <td>{''': 2, '.': 2}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134446</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>{‚Äù,  }</td>\n",
       "      <td>8</td>\n",
       "      <td>[', \", ,, /, /, ., /, ']</td>\n",
       "      <td>{''': 2, '\"': 1, ',': 1, '/': 3, '.': 1}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Syllables  Sentences   Spelling_Mistakes  count_punct  \\\n",
       "155520         29          1              {‚Ä¶,  }            5   \n",
       "210971         28          2           {‚Äô,  , ‚Ä¶}            3   \n",
       "186412         26          1  {‚Ä¶,  , ‚Äú, üèª, üëá, ‚Äù}            3   \n",
       "206715         20          1                 { }            3   \n",
       "207516         21          1                 { }            3   \n",
       "...           ...        ...                 ...          ...   \n",
       "106049         27          1              {‚Ä¶,  }            5   \n",
       "29130           5          1                 { }            3   \n",
       "8357           32          1              {‚Ä¶,  }            4   \n",
       "137882         14          2                 { }            4   \n",
       "134446         13          1              {‚Äù,  }            8   \n",
       "\n",
       "                  acc_punct_list                            acc_punct_dict  \\\n",
       "155520           [', #, ., ,, ']          {''': 2, '#': 1, '.': 1, ',': 1}   \n",
       "210971                 [', ., ']                          {''': 2, '.': 1}   \n",
       "186412                 [', :, ']                          {''': 2, ':': 1}   \n",
       "206715                 [', ., ']                          {''': 2, '.': 1}   \n",
       "207516                 [', ,, ']                          {''': 2, ',': 1}   \n",
       "...                          ...                                       ...   \n",
       "106049           [', ., :, |, ']          {''': 2, '.': 1, ':': 1, '|': 1}   \n",
       "29130                  [', ,, ']                          {''': 2, ',': 1}   \n",
       "8357                [', ', ', ']                                  {''': 4}   \n",
       "137882              [', ., ., ']                          {''': 2, '.': 2}   \n",
       "134446  [', \", ,, /, /, ., /, ']  {''': 2, '\"': 1, ',': 1, '/': 3, '.': 1}   \n",
       "\n",
       "          '    #    .    ,    :   -   !   $    |   ?    /   %    \"   (   &  \\\n",
       "155520  2.0  1.0  1.0  1.0  NaN NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "210971  2.0  NaN  1.0  NaN  NaN NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "186412  2.0  NaN  NaN  NaN  1.0 NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "206715  2.0  NaN  1.0  NaN  NaN NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "207516  2.0  NaN  NaN  1.0  NaN NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "...     ...  ...  ...  ...  ...  ..  ..  ..  ...  ..  ...  ..  ...  ..  ..   \n",
       "106049  2.0  NaN  1.0  NaN  1.0 NaN NaN NaN  1.0 NaN  NaN NaN  NaN NaN NaN   \n",
       "29130   2.0  NaN  NaN  1.0  NaN NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "8357    4.0  NaN  NaN  NaN  NaN NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "137882  2.0  NaN  2.0  NaN  NaN NaN NaN NaN  NaN NaN  NaN NaN  NaN NaN NaN   \n",
       "134446  2.0  NaN  1.0  1.0  NaN NaN NaN NaN  NaN NaN  3.0 NaN  1.0 NaN NaN   \n",
       "\n",
       "         =   @   )   \\   [   ]   *   +  Uppercase  Lowercase  \n",
       "155520 NaN NaN NaN NaN NaN NaN NaN NaN          6         90  \n",
       "210971 NaN NaN NaN NaN NaN NaN NaN NaN          2         95  \n",
       "186412 NaN NaN NaN NaN NaN NaN NaN NaN         13         78  \n",
       "206715 NaN NaN NaN NaN NaN NaN NaN NaN         10         49  \n",
       "207516 NaN NaN NaN NaN NaN NaN NaN NaN          9         49  \n",
       "...     ..  ..  ..  ..  ..  ..  ..  ..        ...        ...  \n",
       "106049 NaN NaN NaN NaN NaN NaN NaN NaN         15         66  \n",
       "29130  NaN NaN NaN NaN NaN NaN NaN NaN          1          9  \n",
       "8357   NaN NaN NaN NaN NaN NaN NaN NaN          3         95  \n",
       "137882 NaN NaN NaN NaN NaN NaN NaN NaN         11         39  \n",
       "134446 NaN NaN NaN NaN NaN NaN NaN NaN          9         38  \n",
       "\n",
       "[2000 rows x 31 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 50:82]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nadia: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import textfeatures as tf\n",
    "import texthero as hero\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df['scores'] = df['text'].apply(lambda text: analyzer.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compound'] = df['scores'].apply(lambda score_dict: score_dict['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos']  = df['scores'].apply(lambda score_dict: score_dict['pos'])\n",
    "df['neg']  = df['scores'].apply(lambda score_dict: score_dict['neg'])\n",
    "df['neu']  = df['scores'].apply(lambda score_dict: score_dict['neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('TextfeaturesTweets2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
